#!/usr/bin/env python3
"""
A*ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ Streamlitå¯è¦–åŒ–ã‚¢ãƒ—ãƒª

ã“ã‚Œã§å®Ÿè¡Œ
streamlit run app.py
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from pathlib import Path
import subprocess
import time
import signal
import psutil

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
        page_title="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
        )


# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
.metric-card {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #ff6b6b;
    
    /* --- ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆä¿®æ­£ã®æ ¸ã¨ãªã‚‹éƒ¨åˆ† --- */
    height: 100%; /* è¦ªè¦ç´ (ã‚«ãƒ©ãƒ )ã®é«˜ã•ä¸€æ¯ã«åºƒãŒã‚‹ */
    display: flex; /* Flexboxã‚’æœ‰åŠ¹åŒ– */
    flex-direction: column; /* è¦ç´ ã‚’ç¸¦ã«ä¸¦ã¹ã‚‹ */
}

.metric-card h3 {
    /* ã‚¿ã‚¤ãƒˆãƒ« (ä¾‹: "è§£æ±ºç‡") */
    margin-bottom: 0.5rem; /* ã‚¿ã‚¤ãƒˆãƒ«ã¨æ•°å€¤ã®é–“ã®ä½™ç™½ */
}

.metric-card h2 {
    /* ãƒ¡ã‚¤ãƒ³ã®æ•°å€¤ (ä¾‹: "83/83") */
    margin-bottom: 0.25rem; /* æ•°å€¤ã¨èª¬æ˜æ–‡ã®é–“ã®ä½™ç™½ */
}

.metric-card p {
    /* èª¬æ˜æ–‡ (ä¾‹: "100.0%") */
    margin-top: auto; /* â˜…â˜…â˜… ã“ã‚ŒãŒé‡è¦: è¦ç´ ã‚’ã‚³ãƒ³ãƒ†ãƒŠã®ä¸‹éƒ¨ã«æŠ¼ã—ã‚„ã‚‹ â˜…â˜…â˜… */
    color: #64748b; /* èª¬æ˜æ–‡ã®æ–‡å­—è‰²ã‚’å°‘ã—è–„ãã™ã‚‹ */
    font-size: 0.9rem; /* èª¬æ˜æ–‡ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å°‘ã—å°ã•ãã™ã‚‹ */
    padding-top: 0.5rem; /* ä¸Šã®è¦ç´ ã¨ã®é–“ã«å°‘ã—ä½™ç™½ã‚’ç¢ºä¿ */
}

/* å„ã‚«ãƒ¼ãƒ‰ã®å·¦æ ç·šã®è‰² */
.success-metric {
    border-left-color: #51cf66;
}
.warning-metric {
    border-left-color: #ffd43b;
}
.info-metric {
    border-left-color: #339af0;
}
</style>
""", unsafe_allow_html=True)

def to_serializable(obj):
    """json.dump ã® default ã§ä½¿ã†: NumPy å‹ â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–å‹"""
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if isinstance(obj, (np.ndarray,)):
        return obj.tolist()
    # ã“ã“ã«æ¥ã‚‹ã®ã¯ str ã‚„ bool ãªã© json ãŒæ‰±ãˆã‚‹å‹
    return obj

def calculate_metrics(df):
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    if df.empty:
        return df

    # æœ€é©è§£æ¨å®šå¼ (ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®ã‚«ã‚¹ã‚¿ãƒ å¼) ã¨ã®å·®åˆ†
    # y = 1/8 * n^3 - n^2 + 7*n - 14
    # çµæœãŒ1æœªæº€ã®å ´åˆã¯1ã¨ã™ã‚‹
    s = df['size'] # For brevity
    calculated_value = (1/8 * s**3) - (s**2) + (7*s) - 14
    df['estimated_optimal'] = np.maximum(1, calculated_value) # np.maximum ã¯ Series ã«ã‚‚å¯¾å¿œ
    df['diff_from_estimated'] = df['num_moves'] - df['estimated_optimal']

    # nÂ²ã¨ã®å·®åˆ†
    df['n_squared'] = df['size'] * df['size']
    df['diff_from_n_squared'] = df['num_moves'] - df['n_squared']

    # æ¢ç´¢åŠ¹ç‡ (solved problems per second based on nodes explored)
    df['search_efficiency'] = df['nodes_explored'] / (df['calculation_time_ms'] / 1000)

    # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå‚è€ƒå€¤ï¼‰- å°ã•ã„ã»ã©è‰¯ã„
    # å„æŒ‡æ¨™ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ã—ã€åˆè¨ˆã™ã‚‹ã€‚
    # calculation_time_ms ã¨ num_moves ã¯å°ã•ã„ã»ã©è‰¯ã„ãŸã‚ã€ãã®ã¾ã¾æ­£è¦åŒ–ã€‚
    # search_efficiency ã¯å¤§ãã„ã»ã©è‰¯ã„ãŸã‚ã€ (1 - æ­£è¦åŒ–å€¤) ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚³ã‚¢ã«å¯„ä¸ã•ã›ã‚‹ã€‚
    # 1e-10 ã¯ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã®å¾®å°å€¤ã€‚
    if len(df) > 1:  # æ­£è¦åŒ–ã®ãŸã‚ã«è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
        time_normalized = (df['calculation_time_ms'] - df['calculation_time_ms'].min()) / \
                          (df['calculation_time_ms'].max() - df['calculation_time_ms'].min() + 1e-10)
        moves_normalized = (df['num_moves'] - df['num_moves'].min()) / \
                           (df['num_moves'].max() - df['num_moves'].min() + 1e-10)
        # search_efficiency ã¯é«˜ã„ã»ã©è‰¯ã„ãŸã‚ã€ã‚¹ã‚³ã‚¢ã¨ã—ã¦ã¯ (1 - æ­£è¦åŒ–å€¤) ã‚’ä½¿ã†ã“ã¨ã§ã€ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã«åˆã‚ã›ã‚‹
        efficiency_normalized = 1 - ((df['search_efficiency'] - df['search_efficiency'].min()) / \
                                     (df['search_efficiency'].max() - df['search_efficiency'].min() + 1e-10))

        df['composite_score'] = time_normalized + moves_normalized + efficiency_normalized
    else:
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯æ­£è¦åŒ–ã§ããªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢ã‚’è¨­å®š
        df['composite_score'] = 1.0

    # Add Manhattan distance diffs if columns are available
    if 'manhattan_min' in df.columns and 'num_moves' in df.columns:
        df['diff_from_manhattan_min'] = df['num_moves'] - df['manhattan_min']
    if 'manhattan_max' in df.columns and 'num_moves' in df.columns:
        df['diff_from_manhattan_max'] = df['num_moves'] - df['manhattan_max']

    return df


def calculate_summary(df):
    """ã‚µãƒãƒªãƒ¼çµ±è¨ˆã‚’è¨ˆç®—"""
    if df.empty or 'solved' not in df.columns:
        return None

    solved_df = df[df['solved'] == True]

    summary = {
            'total_problems': len(df),
            'solved_problems': len(solved_df),
            'total_calculation_time_ms': df['calculation_time_ms'].sum(),
            'avg_calculation_time_ms': df['calculation_time_ms'].mean(),
            'total_actual_time_ms': df['actual_execution_time_ms'].sum(),
            'avg_actual_time_ms': df['actual_execution_time_ms'].mean(),
            'total_nodes_explored': df['nodes_explored'].sum(),
            'avg_nodes_explored': df['nodes_explored'].mean(),
            'avg_moves': df['num_moves'].mean(),
            'avg_diff_from_estimated': df['diff_from_estimated'].mean(),
            'avg_diff_from_n_squared': df['diff_from_n_squared'].mean(),
            'avg_search_efficiency': df['search_efficiency'].mean(),
            'avg_composite_score': df['composite_score'].mean()
            }
    return summary


@st.cache_data
def load_benchmark_results():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã‚€"""
    results_file = Path("benchmark_results.csv")
    summary_file = Path("benchmark_summary.json")
    problems_summary_file = Path("problems_summary.csv")
    problems_summary_df = None

    try:
        if problems_summary_file.exists():
            problems_summary_df = pd.read_csv(problems_summary_file)
            if problems_summary_df.empty:
                st.warning(f"å•é¡Œæ¦‚è¦ãƒ•ã‚¡ã‚¤ãƒ« '{problems_summary_file}' ã¯ç©ºã§ã™ã€‚ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢æ¯”è¼ƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                problems_summary_df = None
        else:
            st.info(f"å•é¡Œæ¦‚è¦ãƒ•ã‚¡ã‚¤ãƒ« '{problems_summary_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢æ¯”è¼ƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    except pd.errors.EmptyDataError: # Should be caught by problems_summary_df.empty check, but as a safeguard
        st.warning(f"å•é¡Œæ¦‚è¦ãƒ•ã‚¡ã‚¤ãƒ« '{problems_summary_file}' ã¯ç©ºã§ã™ã€‚ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢æ¯”è¼ƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        problems_summary_df = None
    except pd.errors.ParserError:
        st.error(f"å•é¡Œæ¦‚è¦ãƒ•ã‚¡ã‚¤ãƒ« '{problems_summary_file}' ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢æ¯”è¼ƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        problems_summary_df = None
    except FileNotFoundError: # Should be caught by .exists(), but as a safeguard
        st.info(f"å•é¡Œæ¦‚è¦ãƒ•ã‚¡ã‚¤ãƒ« '{problems_summary_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢æ¯”è¼ƒã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        problems_summary_df = None


    if not results_file.exists():
        return pd.DataFrame(), None

    try:
        df = pd.read_csv(results_file)
    except pd.errors.EmptyDataError:
        st.warning(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ•ã‚¡ã‚¤ãƒ« '{results_file}' ã¯ç©ºã§ã™ã€‚")
        return pd.DataFrame(), None
    except pd.errors.ParserError:
        st.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ•ã‚¡ã‚¤ãƒ« '{results_file}' ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return pd.DataFrame(), None

    if problems_summary_df is not None and not problems_summary_df.empty and 'problem_id' in df.columns:
        try:
            # Ensure 'problem_id' in df is string before stripping, and handle potential errors if it's not
            if pd.api.types.is_string_dtype(df['problem_id']):
                df['problem_id_numeric'] = df['problem_id'].str.lstrip('p').astype(int)
            elif pd.api.types.is_numeric_dtype(df['problem_id']): # If it's already numeric (e.g. from old file)
                df['problem_id_numeric'] = df['problem_id'].astype(int)
            else: # Try converting to string first
                df['problem_id_numeric'] = df['problem_id'].astype(str).str.lstrip('p').astype(int)

            df = pd.merge(
                df,
                problems_summary_df,
                left_on=['size', 'problem_id_numeric'],
                right_on=['size', 'problem_id'], # Assuming 'problem_id' in problems_summary.csv is already numeric
                how='left',
                suffixes=('', '_summary')
            )
            if 'problem_id_summary' in df.columns:
                df = df.drop(columns=['problem_id_summary'])
            # We might want to keep problem_id_numeric for other uses or drop it:
            # if 'problem_id_numeric' in df.columns:
            #     df = df.drop(columns=['problem_id_numeric'])
        except ValueError as e:
            st.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã® 'problem_id' ã®å½¢å¼ãŒä¸æ­£ãªãŸã‚ã€å•é¡Œæ¦‚è¦ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        except Exception as e: # Catch any other unexpected error during merge
            st.error(f"å•é¡Œæ¦‚è¦ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


    summary = None
    if summary_file.exists():
        try:
            # First, check if the file is not empty to avoid an error
            if summary_file.stat().st_size > 0:
                with summary_file.open('r', encoding='utf-8') as f:
                    summary = json.load(f)
            else:
                st.warning(f"è­¦å‘Š: '{summary_file}' ã¯ç©ºã§ã™ã€‚ç„¡è¦–ã•ã‚Œã¾ã™ã€‚")
        except json.JSONDecodeError as e:
            st.warning(f"'{summary_file}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
            st.info("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å†å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«ä¿®å¾©ï¼ˆä¸Šæ›¸ãï¼‰ã•ã‚Œã¾ã™ã€‚")
            # On error, ensure summary remains None so the app can proceed
            summary = None

    return df, summary


def kill_process_tree(process):
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒ„ãƒªãƒ¼å…¨ä½“ã‚’ç¢ºå®Ÿã«çµ‚äº†ã™ã‚‹"""
    try:
        # psutilã‚’ä½¿ç”¨ã—ã¦æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ã®æƒ…å ±ã‚’å–å¾—
        parent = psutil.Process(process.pid)
        # ãã®ãƒ—ãƒ­ã‚»ã‚¹ã®å­ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†å¸°çš„ã«å…¨ã¦å–å¾—
        children = parent.children(recursive=True)

        # ã¾ãšå­ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰é †ã«çµ‚äº†ã•ã›ã‚‹
        for child in children:
            try:
                child.terminate()  # SIGTERMã‚’é€ä¿¡
            except psutil.NoSuchProcess:
                pass  # ãƒ—ãƒ­ã‚»ã‚¹ãŒæ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

        # æ¬¡ã«è¦ªãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã•ã›ã‚‹
        try:
            parent.terminate()  # SIGTERMã‚’é€ä¿¡
        except psutil.NoSuchProcess:
            pass  # ãƒ—ãƒ­ã‚»ã‚¹ãŒæ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

        # terminateã‚·ã‚°ãƒŠãƒ«ã§çµ‚äº†ã—ãªã‹ã£ãŸãƒ—ãƒ­ã‚»ã‚¹ãŒã„ãªã„ã‹ç¢ºèªã—ã€å¼·åˆ¶çµ‚äº†ã™ã‚‹
        # wait_procsã§ãƒ—ãƒ­ã‚»ã‚¹ã®çµ‚äº†ã‚’å¾…ã¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        gone, still_alive = psutil.wait_procs(children + [parent], timeout=3)
        for p in still_alive: # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾Œã‚‚ç”Ÿå­˜ã—ã¦ã„ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾ã—ã¦
            try:
                p.kill()  # SIGKILLã‚’é€ä¿¡ã—ã¦å¼·åˆ¶çµ‚äº†
            except psutil.NoSuchProcess:
                pass  # ãƒ—ãƒ­ã‚»ã‚¹ãŒæ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„

    except (psutil.NoSuchProcess, psutil.AccessDenied, OSError) as e:
        # æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ãŒå­˜åœ¨ã—ãªã„ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒãªã„ã€ãã®ä»–ã®OSã‚¨ãƒ©ãƒ¼ã®å ´åˆ
        # subprocessã®æ¨™æº–çš„ãªæ–¹æ³•ã§ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚’è©¦ã¿ã‚‹
        st.warning(f"psutilã§ã®ãƒ—ãƒ­ã‚»ã‚¹ãƒ„ãƒªãƒ¼çµ‚äº†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}ã€‚æ¨™æº–çš„ãªçµ‚äº†å‡¦ç†ã‚’è©¦ã¿ã¾ã™ã€‚")
        try:
            process.terminate()  # SIGTERMã‚’é€ä¿¡
            process.wait(timeout=3)  # çµ‚äº†ã‚’å¾…ã¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        except subprocess.TimeoutExpired:
            process.kill()  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã‚‰SIGKILLã‚’é€ä¿¡
            process.wait()
        except OSError as final_e: # killã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆ
            st.error(f"æ¨™æº–çš„ãªãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†å‡¦ç†ã§ã‚‚ã‚¨ãƒ©ãƒ¼: {final_e}ã€‚")
            pass # ã“ã‚Œä»¥ä¸Šã§ãã‚‹ã“ã¨ã¯å°‘ãªã„


def run_single_problem(executable_path, problem_path):
    """å˜ä¸€ã®å•é¡Œã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™"""
    cat_process = None
    astar_process = None

    try:
        start_time = time.perf_counter()

        # cat problem_file | ./astar_manhattan ã®å½¢ã§å®Ÿè¡Œ
        cat_process = subprocess.Popen(
                ["cat", problem_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
                )

        astar_process = subprocess.Popen(
                [executable_path],
                stdin=cat_process.stdout,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
                )

        cat_process.stdout.close()

        stdout, stderr = astar_process.communicate(timeout=60)
        end_time = time.perf_counter()

        execution_time = (end_time - start_time) * 1000  # ãƒŸãƒªç§’

        if astar_process.returncode != 0:
            return None, f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {stderr}"

        try:
            output_data = json.loads(stdout.strip())
            output_data['actual_execution_time_ms'] = execution_time
            output_data['problem_path'] = problem_path
            return output_data, None
        except json.JSONDecodeError as e:
            return None, f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}\nå‡ºåŠ›: {stdout}"

    except subprocess.TimeoutExpired:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®å‡¦ç†ã‚’æ”¹å–„
        error_msg = "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (60ç§’)"

        # ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºå®Ÿã«çµ‚äº†
        if astar_process is not None:
            kill_process_tree(astar_process)
        if cat_process is not None:
            kill_process_tree(cat_process)

        return None, error_msg

    except Exception as e:
        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if astar_process is not None:
            try:
                kill_process_tree(astar_process)
            except:
                pass
        if cat_process is not None:
            try:
                kill_process_tree(cat_process)
            except:
                pass

        return None, f"äºˆæœŸã›ã¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"


def run_benchmark_with_config(executable_path_str, problems_dir_str, problems_per_size, min_size, max_size):
    """è¨­å®šã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œï¼ˆã‚µã‚¤ã‚ºç¯„å›²æŒ‡å®šä»˜ãï¼‰"""
    executable_path = Path(executable_path_str)
    problems_dir = Path(problems_dir_str)

    if not executable_path.exists():
        st.error(f"å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ« '{executable_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    if not problems_dir.is_dir(): # Changed from exists() to is_dir() for clarity
        st.error(f"å•é¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{problems_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    problem_files = []
    # problemsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚©ãƒ«ãƒ€('4x4', '5x5'ãªã©)ã‚’åå‰ã‹ã‚‰ã‚µã‚¤ã‚ºã‚’èª­ã¿å–ã‚Šã‚½ãƒ¼ãƒˆã—ã¦å–å¾—
    size_dirs = sorted(
        [d for d in problems_dir.iterdir() if d.is_dir() and 'x' in d.name],  # 'x' ã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿ã‚’å¯¾è±¡
        key=lambda d: int(d.name.split('x')[0])
    )

    for size_dir in size_dirs:
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå (ä¾‹: "4x4") ã‹ã‚‰ã‚µã‚¤ã‚º (ä¾‹: 4) ã‚’å–å¾—
            current_size = int(size_dir.name.split('x')[0])
            # æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºç¯„å›²å¤–ã§ã‚ã‚Œã°ã€ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å•é¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            if not (min_size <= current_size <= max_size):
                continue
        except (ValueError, IndexError):
            # '4x4' ã®ã‚ˆã†ãªå½¢å¼ã§ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ç„¡è¦–
            continue

        # å„ã‚µã‚¤ã‚ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.jsonï¼‰ã‚’åå‰é †ã§ã‚½ãƒ¼ãƒˆã—ã¦å–å¾—
        files_in_dir = sorted(size_dir.glob("*.json"), key=lambda path: int(path.stem.lstrip('p')))
        # æŒ‡å®šã•ã‚ŒãŸå•é¡Œæ•°ã ã‘ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ã€å…¨ä½“ã®ãƒªã‚¹ãƒˆã«è¿½åŠ 
        problem_files.extend(files_in_dir[:problems_per_size])

    if not problem_files:
        st.error(f"æŒ‡å®šã•ã‚ŒãŸç¯„å›² ({min_size}x{min_size}ã€œ{max_size}x{max_size}) ã§å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«(.json)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    st.info(f"å®Ÿè¡Œã™ã‚‹å•é¡Œæ•°: {len(problem_files)}")

    progress_bar = st.progress(0, text="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œæº–å‚™ä¸­...")
    results = []  # æˆåŠŸã—ãŸå•é¡Œã®çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    failed_problems_details = [] # å¤±æ•—ã—ãŸå•é¡Œã®è©³ç´°ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    start_time = time.time()

    for i, problem_path in enumerate(problem_files):
        progress = (i + 1) / len(problem_files)
        # UIã«é€²æ—ã‚’è¡¨ç¤º (ä¾‹: "å®Ÿè¡Œä¸­ (10/100): p010.json")
        progress_bar.progress(progress, text=f"å®Ÿè¡Œä¸­ ({i + 1}/{len(problem_files)}): {problem_path.name}")

        # å€‹åˆ¥ã®å•é¡Œã‚’å®Ÿè¡Œã—ã€çµæœã¨ã‚¨ãƒ©ãƒ¼(ã‚ã‚Œã°)ã‚’å–å¾—
        result, error = run_single_problem(str(executable_path), str(problem_path))

        if result: # å®Ÿè¡ŒæˆåŠŸæ™‚
            try:
                # å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚µã‚¤ã‚ºã¨å•é¡ŒIDã‚’æŠ½å‡º
                # ä¾‹: problems/4x4/p001.json -> path_parts = ('problems', '4x4', 'p001.json')
                path_parts = problem_path.parts
                size_str = path_parts[-2]  # '4x4'
                problem_id_str = problem_path.stem  # 'p001'
                size = int(size_str.split('x')[0]) # 4

                # çµæœã«IDã¨ã‚µã‚¤ã‚ºæƒ…å ±ã‚’è¿½åŠ ã—ã¦ãƒªã‚¹ãƒˆã«æ ¼ç´
                result['problem_id'] = problem_id_str
                result['size'] = size
                results.append(result)
            except (IndexError, ValueError) as e:
                # ãƒ‘ã‚¹è§£æã«å¤±æ•—ã—ãŸå ´åˆ (é€šå¸¸ã¯èµ·ã“ã‚Šå¾—ãªã„ãŒå¿µã®ãŸã‚)
                st.warning(f"ãƒ‘ã‚¹ '{problem_path}' ã‹ã‚‰ã‚µã‚¤ã‚ºã¾ãŸã¯IDã®è§£æã«å¤±æ•—: {e}")
        else: # å®Ÿè¡Œå¤±æ•—æ™‚
            # UIã«è­¦å‘Šã‚’è¡¨ç¤ºã—ã€å¤±æ•—ãƒªã‚¹ãƒˆã«è©³ç´°ã‚’è¨˜éŒ²
            st.warning(f"å•é¡Œ {problem_path.name} ã§ã‚¨ãƒ©ãƒ¼: {error}")
            failed_problems_details.append({'name': problem_path.name, 'path': str(problem_path), 'error': error})

    end_time = time.time() # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å…¨ä½“ã®çµ‚äº†æ™‚åˆ»

    if results:
        df = pd.DataFrame(results)
        df = calculate_metrics(df)
        results_csv_path = Path("benchmark_results.csv")
        summary_json_path = Path("benchmark_summary.json")
        df.to_csv(results_csv_path, index=False)

        summary = calculate_summary(df)
        if summary:
            with summary_json_path.open('w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=to_serializable)

        progress_bar.progress(1.0, text="å®Œäº†ï¼")
        st.success(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå®Œäº†ï¼ (åˆè¨ˆæ™‚é–“: {end_time - start_time:.2f}ç§’)")
        st.success(f"æˆåŠŸ: {len(results)}/{len(problem_files)} å•é¡Œ")

        if failed_problems_details:
            st.error(f"å®Ÿè¡Œä¸­ã« {len(failed_problems_details)} ä»¶ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            with st.expander("âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º", expanded=True):
                for failure in failed_problems_details:
                    st.markdown(f"**å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«:** `{failure['name']}`")
                    st.markdown(f"**ãƒ‘ã‚¹:** `{failure['path']}`")
                    st.markdown(f"**ã‚¨ãƒ©ãƒ¼å†…å®¹:**")
                    st.error(f"{failure['error']}") # Using st.error for the message itself
                    st.markdown("---") # Separator

        load_benchmark_results.clear()
        st.rerun()
    else:
        st.error("å®Ÿè¡Œã«æˆåŠŸã—ãŸå•é¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        if failed_problems_details: # Also show errors if no problems succeeded
            st.error(f"å®Ÿè¡Œä¸­ã« {len(failed_problems_details)} ä»¶ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            with st.expander("âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º", expanded=True):
                for failure in failed_problems_details:
                    st.markdown(f"**å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«:** `{failure['name']}`")
                    st.markdown(f"**ãƒ‘ã‚¹:** `{failure['path']}`")
                    st.markdown(f"**ã‚¨ãƒ©ãƒ¼å†…å®¹:**")
                    st.error(f"{failure['error']}")
                    st.markdown("---")


def display_metrics_overview(summary):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦ã‚’è¡¨ç¤º"""
    st.header("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦")

    col1, col2, col3, col4 = st.columns(4)
    solved_percentage = (summary['solved_problems'] / summary['total_problems'] * 100) if summary['total_problems'] > 0 else 0

    with col1:
        st.markdown(f"""
        <div class="metric-card success-metric">
            <h3>è§£æ±ºç‡</h3>
            <h2>{summary.get('solved_problems', 0)}/{summary.get('total_problems', 0)}</h2>
            <p>{solved_percentage:.1f}%</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown(f"""
        <div class="metric-card info-metric">
            <h3>å¹³å‡è¨ˆç®—æ™‚é–“</h3>
            <h2>{summary.get('avg_calculation_time_ms', 0):.2f}ms</h2>
            <p>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å†…éƒ¨</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown(f"""
        <div class="metric-card warning-metric">
            <h3>å¹³å‡æ¢ç´¢ãƒãƒ¼ãƒ‰</h3>
            <h2>{summary.get('avg_nodes_explored', 0):,.0f}</h2>
            <p>nodes</p>
        </div>
        """, unsafe_allow_html=True)

    with col4:
        st.markdown(f"""
        <div class="metric-card">
            <h3>å¹³å‡æ‰‹æ•°</h3>
            <h2>{summary.get('avg_moves', 0):.1f}</h2>
            <p>moves</p>
        </div>
        """, unsafe_allow_html=True)


def plot_time_performance(df):
    """æ™‚é–“æ€§èƒ½ã®å¯è¦–åŒ–"""
    st.subheader("â±ï¸ æ™‚é–“æ€§èƒ½åˆ†æ")

    col1, col2 = st.columns(2)

    with col1:
        size_time = df.groupby('size')['calculation_time_ms'].agg(['mean', 'std']).reset_index()
        fig = px.line(size_time, x='size', y='mean',
                      title='ã‚µã‚¤ã‚ºåˆ¥å¹³å‡è¨ˆç®—æ™‚é–“',
                      labels={'mean': 'å¹³å‡è¨ˆç®—æ™‚é–“ (ms)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        fig = px.histogram(df, x='calculation_time_ms',
                           title='è¨ˆç®—æ™‚é–“åˆ†å¸ƒ',
                           labels={'calculation_time_ms': 'è¨ˆç®—æ™‚é–“ (ms)', 'count': 'å•é¡Œæ•°'},
                           nbins=50)
        st.plotly_chart(fig, use_container_width=True)


def plot_search_performance(df):
    """æ¢ç´¢æ€§èƒ½ã®å¯è¦–åŒ–"""
    st.subheader("ğŸ” æ¢ç´¢æ€§èƒ½åˆ†æ")

    col1, col2 = st.columns(2)

    with col1:
        fig = px.box(df, x='size', y='nodes_explored',
                     title='ã‚µã‚¤ã‚ºåˆ¥æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°åˆ†å¸ƒ',
                     labels={'nodes_explored': 'æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        fig = px.scatter(df, x='size', y='search_efficiency',
                         color='calculation_time_ms',
                         title='æ¢ç´¢åŠ¹ç‡ vs ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                         labels={'search_efficiency': 'æ¢ç´¢åŠ¹ç‡ (nodes/sec)',
                                 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'calculation_time_ms': 'è¨ˆç®—æ™‚é–“ (ms)'},
                         color_continuous_scale='viridis')
        st.plotly_chart(fig, use_container_width=True)


def plot_solution_quality(df):
    """è§£ã®å“è³ªåˆ†æ"""
    st.subheader("ğŸ¯ è§£ã®å“è³ªåˆ†æ")
    st.caption("""
    ã“ã“ã§ã¯ã€å®Ÿéš›ã®è§£æ±ºæ‰‹æ•°ãŒç†è«–çš„ãªæ¨å®šå€¤ã‚„é™ç•Œå€¤ã¨ã©ã®ç¨‹åº¦ç•°ãªã‚‹ã‹ã‚’åˆ†æã—ã¾ã™ã€‚
    - ã€Œæ¨å®šæœ€é©è§£ã€ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®è¨ˆç®—å¼ (y = 1/8 nÂ³ - nÂ² + 7n - 14, çµæœã¯æœ€å°1) ã«åŸºã¥ãç†è«–çš„ãªæœ€é©æ‰‹æ•°ã§ã™ã€‚ã‚°ãƒ©ãƒ•ã§ã¯ã“ã®æ¨å®šå€¤ã‹ã‚‰ã®å·®åˆ†ï¼ˆå®Ÿéš›ã®æ‰‹æ•° - æ¨å®šæœ€é©è§£ï¼‰ã‚’ç¤ºã—ã¾ã™ã€‚
    - ã€Œç†è«–æœ€å¤§å€¤(nÂ²)ã€ã¯ã€æ‰‹æ•°ã®å˜ç´”ãªä¸Šé™ã®ç›®å®‰ã§ã™ã€‚ã‚°ãƒ©ãƒ•ã§ã¯ã“ã®nÂ²ã‹ã‚‰ã®å·®åˆ†ï¼ˆå®Ÿéš›ã®æ‰‹æ•° - nÂ²ï¼‰ã‚’ç¤ºã—ã¾ã™ã€‚
    """)
    if 'diff_from_estimated' not in df.columns:
        st.info("è§£ã®å“è³ªã‚’åˆ†æã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return

    col1, col2 = st.columns(2)
    with col1:
        fig = px.scatter(df, x='size', y='diff_from_estimated',
                         color='num_moves',
                         title='æ¨å®šæœ€é©è§£ (ã‚«ã‚¹ã‚¿ãƒ å¼) ã‹ã‚‰ã®å·®åˆ†',
                         labels={'diff_from_estimated': 'å·®åˆ† (moves)',
                                 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'num_moves': 'å®Ÿéš›ã®æ‰‹æ•°'},
                         color_continuous_scale='RdYlBu_r')
        fig.add_hline(y=0, line_dash="dash", line_color="red", annotation_text="ç†è«–æœ€é©å€¤")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        fig = px.scatter(df, x='size', y='diff_from_n_squared',
                         color='composite_score',
                         title='ç†è«–æœ€å¤§å€¤(nÂ²)ã‹ã‚‰ã®å·®åˆ†',
                         labels={'diff_from_n_squared': 'å·®åˆ† (moves)',
                                 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'composite_score': 'ç·åˆã‚¹ã‚³ã‚¢'},
                         color_continuous_scale='viridis')
        st.plotly_chart(fig, use_container_width=True)


def plot_manhattan_comparison(df):
    """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã¨ã®æ¯”è¼ƒ"""
    if all(col in df.columns for col in ['diff_from_manhattan_min', 'diff_from_manhattan_max']):
        st.subheader("ğŸ“ ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒ")
        st.caption("""
        ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã¯ã€å„ç‰Œã®ç¾åœ¨ä½ç½®ã‹ã‚‰ç›®çš„ä½ç½®ã¾ã§ã®ç¸¦æ¨ªã®ç§»å‹•è·é›¢ã®ç·å’Œã«é–¢é€£ã™ã‚‹ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§ã™ã€‚
        ã“ã“ã§ã¯ã€A*æ¢ç´¢ã§å¾—ã‚‰ã‚ŒãŸå®Ÿéš›ã®è§£æ±ºæ‰‹æ•°ãŒã€å•é¡Œã®åˆæœŸçŠ¶æ…‹ã®ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹ç†è«–çš„ãªæ‰‹æ•°ç¯„å›²ï¼ˆæœ€å°ãƒ»æœ€å¤§æ¨å®šå€¤ï¼‰ã¨ã©ã®ç¨‹åº¦ç•°ãªã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯é–¢æ•°ï¼ˆã“ã®å ´åˆã¯ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã«é–¢é€£ã™ã‚‹ä½•ã‹ï¼‰ã®å“è³ªã‚„ã€è§£ãŒã“ã‚Œã‚‰ã®æ¨å®šç¯„å›²å†…ã«åã¾ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
        """)
        col1, col2 = st.columns(2)
        with col1:
            fig = px.box(df, x='size', y='diff_from_manhattan_min',
                         title='ãƒãƒ³ãƒãƒƒã‚¿ãƒ³æœ€å°å€¤ã‹ã‚‰ã®å·®åˆ†åˆ†å¸ƒ',
                         labels={'diff_from_manhattan_min': 'å·®åˆ† (moves)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
            st.plotly_chart(fig, use_container_width=True)
        with col2:
            fig = px.box(df, x='size', y='diff_from_manhattan_max',
                         title='ãƒãƒ³ãƒãƒƒã‚¿ãƒ³æœ€å¤§å€¤ã‹ã‚‰ã®å·®åˆ†åˆ†å¸ƒ',
                         labels={'diff_from_manhattan_max': 'å·®åˆ† (moves)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
            st.plotly_chart(fig, use_container_width=True)


def plot_comprehensive_analysis(df):
    """ç·åˆåˆ†æ (å„æŒ‡æ¨™ã‚’0-100ã®ã‚¹ã‚³ã‚¢ã«æ­£è¦åŒ–)"""
    st.subheader("ğŸ“ˆ ç·åˆåˆ†æ")
    st.info("å„æŒ‡æ¨™ã‚’0-100ã®ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚å€¤ãŒ100ã«è¿‘ã„ã»ã©ã€ãã®ã‚µã‚¤ã‚ºã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚")

    # NaNã‚’å«ã‚€è¡Œã¯é›†è¨ˆã‹ã‚‰é™¤å¤–
    agg_df = df.dropna(subset=['calculation_time_ms', 'nodes_explored', 'num_moves'])

    if agg_df.empty:
        st.warning("ç·åˆåˆ†æã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    size_summary = agg_df.groupby('size').agg({
        'calculation_time_ms': 'mean',
        'nodes_explored': 'mean',
        'num_moves': 'mean',
        'search_efficiency': 'mean',
        'composite_score': 'mean'
        }).reset_index()

    # æ­£è¦åŒ–å¯¾è±¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãã®è©•ä¾¡ã‚¿ã‚¤ãƒ— ('lower_is_better' ã¾ãŸã¯ 'higher_is_better')
    metrics_to_scale = {
            'calculation_time_ms': 'lower_is_better',  # è¨ˆç®—æ™‚é–“ã¯çŸ­ã„ã»ã©è‰¯ã„
            'nodes_explored': 'lower_is_better',       # æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°ã¯å°‘ãªã„ã»ã©è‰¯ã„
            'num_moves': 'lower_is_better',            # æ‰‹æ•°ã¯å°‘ãªã„ã»ã©è‰¯ã„
            'composite_score': 'lower_is_better',      # ç·åˆã‚¹ã‚³ã‚¢ã¯å°ã•ã„ã»ã©è‰¯ã„
            'search_efficiency': 'higher_is_better'    # æ¢ç´¢åŠ¹ç‡ã¯é«˜ã„ã»ã©è‰¯ã„
            }

    scaled_summary = size_summary.copy()

    for metric, scale_type in metrics_to_scale.items():
        if metric in scaled_summary.columns:
            min_val = scaled_summary[metric].min()
            max_val = scaled_summary[metric].max()

            # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã®ãƒã‚§ãƒƒã‚¯ (minã¨maxãŒåŒã˜å ´åˆã€å…¨ã¦ã®å€¤ãŒåŒã˜ãªã®ã§ã‚¹ã‚³ã‚¢ã¯100ã¨ã™ã‚‹)
            if (max_val - min_val) == 0:
                scaled_summary[metric] = 100.0  # å…¨ã¦åŒã˜å€¤ãªã‚‰æº€ç‚¹ï¼ˆã¾ãŸã¯ä¸­é–“ç‚¹ã€ã“ã“ã§ã¯100ï¼‰
                continue

            # Min-Max ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° ã‚’0-100ã®ç¯„å›²ã§è¡Œã†
            if scale_type == 'lower_is_better':
                # å€¤ãŒå°ã•ã„ã»ã©ã‚¹ã‚³ã‚¢ãŒ100ã«è¿‘ã¥ãã‚ˆã†ã«æ­£è¦åŒ–
                # (ä¾‹: min=10, max=110 ã®æ™‚ã€å€¤ãŒ10ãªã‚‰ã‚¹ã‚³ã‚¢100ã€å€¤ãŒ110ãªã‚‰ã‚¹ã‚³ã‚¢0)
                scaled_summary[metric] = 100 * (1 - (scaled_summary[metric] - min_val) / (max_val - min_val))
            else: # 'higher_is_better'
                # å€¤ãŒå¤§ãã„ã»ã©ã‚¹ã‚³ã‚¢ãŒ100ã«è¿‘ã¥ãã‚ˆã†ã«æ­£è¦åŒ–
                # (ä¾‹: min=10, max=110 ã®æ™‚ã€å€¤ãŒ10ãªã‚‰ã‚¹ã‚³ã‚¢0ã€å€¤ãŒ110ãªã‚‰ã‚¹ã‚³ã‚¢100)
                scaled_summary[metric] = 100 * ((scaled_summary[metric] - min_val) / (max_val - min_val))

    # è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é †ç•ªã¨åå‰ã‚’å®šç¾©
    display_metrics = {
            'calculation_time_ms': 'æ™‚é–“åŠ¹ç‡ã‚¹ã‚³ã‚¢',
            'nodes_explored': 'æ¢ç´¢ç©ºé–“ã‚¹ã‚³ã‚¢',
            'num_moves': 'è§£å“è³ªã‚¹ã‚³ã‚¢',
            'search_efficiency': 'æ¢ç´¢åŠ¹ç‡ã‚¹ã‚³ã‚¢',
            'composite_score': 'ç·åˆã‚¹ã‚³ã‚¢'
            }

    available_metrics = [m for m in display_metrics.keys() if m in scaled_summary.columns]

    if available_metrics:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚µã‚¤ã‚ºã«è¨­å®šã—ã€è¡¨ç¤ºã™ã‚‹åˆ—ã‚’é¸æŠ
        heatmap_data = scaled_summary.set_index('size')[available_metrics].T
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆYè»¸ã®ãƒ©ãƒ™ãƒ«ï¼‰ã‚’æ—¥æœ¬èªã«ç½®æ›
        heatmap_data = heatmap_data.rename(index=display_metrics)

        fig = px.imshow(heatmap_data,
                        title='ã‚µã‚¤ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
                        labels={'x': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º', 'y': 'è©•ä¾¡æŒ‡æ¨™', 'color': 'ã‚¹ã‚³ã‚¢ (0-100)'},
                        color_continuous_scale='RdYlGn', # èµ¤(æ‚ªã„) -> é»„ -> ç·‘(è‰¯ã„) ã®ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
                        aspect="auto"
                        )
        st.plotly_chart(fig, use_container_width=True)
def display_detailed_table(df):
    """è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«"""
    st.subheader("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿")
    st.caption("""
    **è¡¨ã®ä¸»ãªæŒ‡æ¨™ã®èª¬æ˜:**
    - **size:** ãƒ‘ã‚ºãƒ«ã®ã‚µã‚¤ã‚º (ä¾‹: 4x4ã®å ´åˆ, 4)ã€‚
    - **problem_id:** å•é¡Œã®è­˜åˆ¥å­ã€‚
    - **solved:** å•é¡ŒãŒè§£æ±ºã•ã‚ŒãŸã‹ã©ã†ã‹ (True/False)ã€‚
    - **num_moves:** è§£æ±ºã¾ã§ã®å®Ÿéš›ã®æ‰‹æ•°ã€‚
    - **calculation_time_ms:** ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹è¨ˆç®—æ™‚é–“ (ãƒŸãƒªç§’)ã€‚
    - **nodes_explored:** æ¢ç´¢ä¸­ã«å±•é–‹ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®ç·æ•°ã€‚
    - **search_efficiency:** æ¢ç´¢åŠ¹ç‡ (ãƒãƒ¼ãƒ‰/ç§’)ã€‚
    - **diff_from_estimated:** ã€Œæ¨å®šæœ€é©è§£ (ã‚«ã‚¹ã‚¿ãƒ å¼: 1/8 nÂ³ - nÂ² + 7n - 14, æœ€å°1)ã€ã‹ã‚‰ã®å®Ÿéš›ã®æ‰‹æ•°ã®å·®ã€‚
    - **composite_score:** è¨ˆç®—æ™‚é–“, æ‰‹æ•°, æ¢ç´¢åŠ¹ç‡ã‚’æ­£è¦åŒ–ã—ã¦çµ„ã¿åˆã‚ã›ãŸç·åˆã‚¹ã‚³ã‚¢ï¼ˆã“ã®ã‚¹ã‚³ã‚¢ãŒå°ã•ã„ã»ã©è‰¯ã„ã¨è©•ä¾¡ã•ã‚Œã¾ã™ï¼‰ã€‚
    - **error:** å•é¡Œè§£æ±ºã«å¤±æ•—ã—ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚
    (ä¸Šè¨˜ã«åŠ ãˆã¦, `problems_summary.csv` ã¨ã®ãƒãƒ¼ã‚¸ã«æˆåŠŸã—ãŸå ´åˆ, `manhattan_min`, `manhattan_max` ãªã©ã®åˆ—ã‚‚è¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚)
    """)

    col1, col2, col3 = st.columns(3)
    with col1:
        size_filter = st.multiselect("ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚ºé¸æŠ",
                                     options=sorted(df['size'].dropna().unique().astype(int)),
                                     default=sorted(df['size'].dropna().unique().astype(int)))
    with col2:
        # NaNã‚’ç„¡è¦–ã—ã¦æœ€å°ãƒ»æœ€å¤§ã‚’è¨ˆç®—
        min_time, max_time = float(df['calculation_time_ms'].min()), float(df['calculation_time_ms'].max())
        time_range = st.slider("è¨ˆç®—æ™‚é–“ç¯„å›² (ms)", min_value=min_time, max_value=max_time, value=(min_time, max_time))

    with col3:
        # ## å¤‰æ›´: ãƒ•ã‚£ãƒ«ã‚¿ã®é¸æŠè‚¢ã‚’å¢—ã‚„ã™ ##
        solved_filter = st.selectbox("è¡¨ç¤ºå¯¾è±¡", options=["ã™ã¹ã¦", "è§£æ±ºæ¸ˆã¿ã®ã¿", "å¤±æ•—ã®ã¿"], index=0)

    filtered_df = df[df['size'].isin(size_filter)]

    # æ™‚é–“ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è§£æ±ºæ¸ˆã¿ã®ã‚‚ã®ã«ã®ã¿é©ç”¨
    solved_part = filtered_df[filtered_df['solved'] == True]
    failed_part = filtered_df[filtered_df['solved'] == False]

    solved_part = solved_part[
            (solved_part['calculation_time_ms'] >= time_range[0]) &
            (solved_part['calculation_time_ms'] <= time_range[1])
            ]

    filtered_df = pd.concat([solved_part, failed_part])

    if solved_filter == "è§£æ±ºæ¸ˆã¿ã®ã¿":
        filtered_df = filtered_df[filtered_df['solved'] == True]
    elif solved_filter == "å¤±æ•—ã®ã¿":
        filtered_df = filtered_df[filtered_df['solved'] == False]


    # ## å¤‰æ›´: erroråˆ—ã‚’è¿½åŠ  ##
    display_columns = [
            'size', 'problem_id', 'solved', 'num_moves', 'calculation_time_ms',
            'nodes_explored', 'search_efficiency',
            'diff_from_estimated', 'composite_score', 'error'
            ]
    available_columns = [col for col in display_columns if col in filtered_df.columns]

    # NaNã‚’ 'N/A' ã«ç½®æ›ã—ã¦è¡¨ç¤º
    st.dataframe(filtered_df[available_columns].fillna('N/A').round(4), use_container_width=True, height=400)
    st.write(f"è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿æ•°: {len(filtered_df)} / {len(df)}")

def main():
    st.title("ğŸ” é«˜å°‚ãƒ—ãƒ­ã‚³ãƒ³ç«¶æŠ€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

    st.sidebar.title("âš™ï¸ æ“ä½œãƒ‘ãƒãƒ«")
    executable_path_str = st.sidebar.text_input("å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹", value="./astar_manhattan")
    problems_dir_str = st.sidebar.text_input("å•é¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", value="problems")
    executable_path = Path(executable_path_str)
    problems_dir = Path(problems_dir_str)

    # ## è¿½åŠ : å•é¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªã‚µã‚¤ã‚ºã®ç¯„å›²ã‚’è‡ªå‹•æ¤œå‡º ##
    min_avail = 4
    max_avail = 24 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    # problem_path_obj = Path(problems_dir) # Already a Path object
    if problems_dir.exists() and problems_dir.is_dir():
        available_sizes = []
        for d in problems_dir.iterdir():
            if d.is_dir():
                try:
                    size = int(d.name.split('x')[0])
                    available_sizes.append(size)
                except (ValueError, IndexError):
                    pass # '4x4'ã®ã‚ˆã†ãªåå‰ã§ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ç„¡è¦–
        if available_sizes:
            min_avail = min(available_sizes)
            max_avail = max(available_sizes)

    # ## è¿½åŠ : ã‚µã‚¤ã‚ºç¯„å›²ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®å…¥åŠ›æ¬„ ##
    st.sidebar.markdown("---")
    st.sidebar.markdown("##### å®Ÿè¡Œç¯„å›²ã®è¨­å®š")
    col1, col2 = st.sidebar.columns(2)
    with col1:
        min_size = st.number_input("æœ€å°ã‚µã‚¤ã‚º", min_value=min_avail, max_value=max_avail, value=min_avail, step=2)
    with col2:
        max_size = st.number_input("æœ€å¤§ã‚µã‚¤ã‚º", min_value=min_avail, max_value=max_avail, value=max_avail, step=2)

    if min_size > max_size:
        st.sidebar.error("æœ€å°ã‚µã‚¤ã‚ºãŒæœ€å¤§ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚")

    problems_per_size = st.sidebar.number_input(
            "å„ã‚µã‚¤ã‚ºã§å®Ÿè¡Œã™ã‚‹å•é¡Œæ•°",
            min_value=1,
            value=5,
            step=1,
            help="å„ã‚µã‚¤ã‚ºï¼ˆ4x4, 5x5...ï¼‰ã”ã¨ã«ã€ã“ã“ã§æŒ‡å®šã—ãŸæ•°ã®å•é¡Œã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åé †ã«å–å¾—ã—ã¾ã™ã€‚"
            )
    st.sidebar.markdown("---")

    # ## å¤‰æ›´: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œãƒœã‚¿ãƒ³ã®ãƒ­ã‚¸ãƒƒã‚¯ ##
    if st.sidebar.button("ğŸš€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ", type="primary", disabled=(min_size > max_size)):
        # ## å¤‰æ›´: min_sizeã¨max_sizeã‚’å¼•æ•°ã«è¿½åŠ  ##
        run_benchmark_with_config(str(executable_path), str(problems_dir), problems_per_size, min_size, max_size)

    df, summary = load_benchmark_results()

    if df.empty:
        st.warning("âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€ã¾ãšãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    st.sidebar.success(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    st.sidebar.info(f"å•é¡Œæ•°: {len(df)}")
    st.sidebar.info(f"ã‚µã‚¤ã‚ºç¯„å›²: {df['size'].min()}x{df['size'].min()} - {df['size'].max()}x{df['size'].max()}")

    if summary:
        display_metrics_overview(summary)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "â±ï¸ æ™‚é–“æ€§èƒ½", "ğŸ” æ¢ç´¢æ€§èƒ½", "ğŸ¯ è§£å“è³ª", "ğŸ“ˆ ç·åˆåˆ†æ", "ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿"
        ])

    with tab1:
        plot_time_performance(df)
    with tab2:
        plot_search_performance(df)
    with tab3:
        plot_solution_quality(df)
        plot_manhattan_comparison(df)
    with tab4:
        plot_comprehensive_analysis(df)
    with tab5:
        display_detailed_table(df)

    st.markdown("---")
    st.markdown("ğŸš€ A*ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  | Built with Streamlit")

if __name__ == "__main__":
    main()
