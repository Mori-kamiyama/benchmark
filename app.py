#!/usr/bin/env python3
"""
A*ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ Streamlitå¯è¦–åŒ–ã‚¢ãƒ—ãƒª

ã“ã‚Œã§å®Ÿè¡Œ
streamlit run app.py
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from pathlib import Path
import subprocess
import time
import datetime  # æ—¥æ™‚ã‚’æ‰±ã†ãŸã‚ã«è¿½åŠ 
import signal
import psutil
import glob  # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã®ãŸã‚ã«è¿½åŠ 

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
.metric-card {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #ff6b6b;
    height: 100%;
    display: flex;
    flex-direction: column;
}
.metric-card h3 {
    margin-bottom: 0.5rem;
}
.metric-card h2 {
    margin-bottom: 0.25rem;
}
.metric-card p {
    margin-top: auto;
    color: #64748b;
    font-size: 0.9rem;
    padding-top: 0.5rem;
}
.success-metric { border-left-color: #51cf66; }
.warning-metric { border-left-color: #ffd43b; }
.info-metric { border-left-color: #339af0; }
</style>
""", unsafe_allow_html=True)


def to_serializable(obj):
    """json.dump ã® default ã§ä½¿ã†: NumPy å‹ â†’ ãƒã‚¤ãƒ†ã‚£ãƒ–å‹"""
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if isinstance(obj, (np.ndarray,)):
        return obj.tolist()
    return obj


def calculate_metrics(df):
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    if df.empty:
        return df

    # æ—¢å­˜ã®è¨ˆç®—ã«åŠ ãˆã¦ã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
    df_copy = df.copy()
    df_copy['estimated_optimal'] = (3 * df_copy['size'] * df_copy['size']) / 8
    df_copy['diff_from_estimated'] = df_copy['num_moves'] - df_copy['estimated_optimal']
    df_copy['n_squared'] = df_copy['size'] * df_copy['size']
    df_copy['diff_from_n_squared'] = df_copy['num_moves'] - df_copy['n_squared']
    df_copy['search_efficiency'] = df_copy['nodes_explored'] / (df_copy['calculation_time_ms'] / 1000)

    if len(df_copy) > 1:
        time_normalized = (df_copy['calculation_time_ms'] - df_copy['calculation_time_ms'].min()) / (
                df_copy['calculation_time_ms'].max() - df_copy['calculation_time_ms'].min() + 1e-10)
        moves_normalized = (df_copy['num_moves'] - df_copy['num_moves'].min()) / (
                df_copy['num_moves'].max() - df_copy['num_moves'].min() + 1e-10)
        efficiency_normalized = 1 - ((df_copy['search_efficiency'] - df_copy['search_efficiency'].min()) / (
                df_copy['search_efficiency'].max() - df_copy['search_efficiency'].min() + 1e-10))
        df_copy['composite_score'] = time_normalized + moves_normalized + efficiency_normalized
    else:
        df_copy['composite_score'] = 1.0

    return df_copy


def calculate_summary(df):
    """ã‚µãƒãƒªãƒ¼çµ±è¨ˆã‚’è¨ˆç®—"""
    if df.empty or 'solved' not in df.columns:
        return None

    solved_df = df[df['solved'] == True]

    summary = {
        'total_problems': len(df),
        'solved_problems': len(solved_df),
        'total_calculation_time_ms': df['calculation_time_ms'].sum(),
        'avg_calculation_time_ms': df['calculation_time_ms'].mean(),
        'total_actual_time_ms': df['actual_execution_time_ms'].sum(),
        'avg_actual_time_ms': df['actual_execution_time_ms'].mean(),
        'total_nodes_explored': df['nodes_explored'].sum(),
        'avg_nodes_explored': df['nodes_explored'].mean(),
        'avg_moves': df['num_moves'].mean()
    }

    # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªåˆ—ã‚‚å¹³å‡ã‚’è¨ˆç®—
    for col in ['diff_from_estimated', 'diff_from_n_squared', 'search_efficiency', 'composite_score']:
        if col in df.columns:
            summary[f'avg_{col}'] = df[col].mean()

    return summary


@st.cache_data
def load_benchmark_results(results_csv_path):
    """ã€å¤‰æ›´ã€‘æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã‚€"""
    if not results_csv_path or not os.path.exists(results_csv_path):
        return pd.DataFrame(), None

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’CSVãƒ‘ã‚¹ã‹ã‚‰ç”Ÿæˆ
    summary_file = str(Path(results_csv_path).with_suffix('.json'))

    df = pd.read_csv(results_csv_path)

    summary = None
    if os.path.exists(summary_file):
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                if os.path.getsize(summary_file) > 0:
                    summary = json.load(f)
                else:
                    st.warning(f"è­¦å‘Š: '{summary_file}' ã¯ç©ºã§ã™ã€‚")
        except json.JSONDecodeError as e:
            st.warning(f"'{summary_file}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
            summary = None  # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å‹•ä½œã‚’ç¶™ç¶š

    # èª­ã¿è¾¼ã‚“ã å¾Œã«å†åº¦ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆJSONãŒãªã„å ´åˆã®ãŸã‚ï¼‰
    if not summary:
        recalculated_df = calculate_metrics(df.copy())
        summary = calculate_summary(recalculated_df)

    return df, summary


def kill_process_tree(process):
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒ„ãƒªãƒ¼å…¨ä½“ã‚’ç¢ºå®Ÿã«çµ‚äº†ã™ã‚‹"""
    try:
        parent = psutil.Process(process.pid)
        children = parent.children(recursive=True)
        for child in children:
            try:
                child.terminate()
            except psutil.NoSuchProcess:
                pass
        parent.terminate()
        gone, still_alive = psutil.wait_procs(children + [parent], timeout=3)
        for p in still_alive:
            try:
                p.kill()
            except psutil.NoSuchProcess:
                pass
    except (psutil.NoSuchProcess, psutil.AccessDenied, OSError):
        try:
            process.terminate()
            process.wait(timeout=3)
        except subprocess.TimeoutExpired:
            process.kill()
            process.wait()
        except OSError:
            pass


def get_timeout_config_path(executable_path: str) -> str:
    """å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹"""
    exec_path = Path(executable_path)
    config_filename = f"{exec_path.stem}_config.json"
    return str(exec_path.parent / config_filename)


def load_timeout_from_config(config_path: str) -> int:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’èª­ã¿è¾¼ã‚€"""
    default_timeout = 60
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        timeout = config_data.get("timeout")
        if isinstance(timeout, int) and 10 <= timeout <= 300:
            return timeout
        else:
            # Log or warn about invalid timeout value, then return default
            st.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ãŒç„¡åŠ¹ã§ã™ (å€¤: {timeout})ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ {default_timeout}ç§’ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return default_timeout
    except FileNotFoundError:
        # Log or inform that config file was not found, using default
        # st.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ {default_timeout}ç§’ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default_timeout
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        # Log or warn about error in config file
        st.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ {default_timeout}ç§’ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return default_timeout


def save_timeout_to_config(config_path: str, timeout_value: int):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        # Ensure the directory exists
        Path(config_path).parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({"timeout": timeout_value}, f, indent=2)
        st.toast(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š ({timeout_value}ç§’) ã‚’ '{config_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚", icon="âœ…")
    except IOError as e:
        st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    except Exception as e:
        st.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®ä¿å­˜ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def run_single_problem(executable_path, problem_path):
    """å˜ä¸€ã®å•é¡Œã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™"""
    config_path = get_timeout_config_path(executable_path)
    configured_timeout = load_timeout_from_config(config_path)

    cat_process = None
    astar_process = None
    try:
        start_time = time.perf_counter()
        cat_process = subprocess.Popen(["cat", problem_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        astar_process = subprocess.Popen([executable_path], stdin=cat_process.stdout, stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE, text=True)
        cat_process.stdout.close()
        stdout, stderr = astar_process.communicate(timeout=configured_timeout)
        end_time = time.perf_counter()
        execution_time = (end_time - start_time) * 1000

        if astar_process.returncode != 0:
            return None, f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {stderr}"

        try:
            output_data = json.loads(stdout.strip())
            output_data['actual_execution_time_ms'] = execution_time
            output_data['problem_path'] = problem_path
            return output_data, None
        except json.JSONDecodeError as e:
            return None, f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}\nå‡ºåŠ›: {stdout}"

    except subprocess.TimeoutExpired:
        error_msg = f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({configured_timeout}ç§’)"
        if astar_process: kill_process_tree(astar_process)
        if cat_process: kill_process_tree(cat_process)
        return None, error_msg
    except Exception as e:
        if astar_process: kill_process_tree(astar_process)
        if cat_process: kill_process_tree(cat_process)
        return None, f"äºˆæœŸã›ã¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"


def run_benchmark_with_config(executable_path, problems_dir, problems_per_size, min_size, max_size):
    """ã€å¤‰æ›´ã€‘è¨­å®šã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã€çµæœã‚’åå‰ã‚’ä»˜ã‘ã¦ä¿å­˜"""
    if not os.path.exists(executable_path):
        st.error(f"å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ« '{executable_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    if not os.path.exists(problems_dir):
        st.error(f"å•é¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{problems_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    problem_files = []
    size_dirs = sorted([d for d in Path(problems_dir).iterdir() if d.is_dir()], key=lambda d: int(d.name.split('x')[0]))
    for size_dir in size_dirs:
        try:
            current_size = int(size_dir.name.split('x')[0])
            if not (min_size <= current_size <= max_size):
                continue
        except (ValueError, IndexError):
            continue
        files_in_dir = sorted(size_dir.glob("*.json"), key=lambda path: int(path.stem.lstrip('p')))
        problem_files.extend(files_in_dir[:problems_per_size])

    if not problem_files:
        st.error(f"æŒ‡å®šã•ã‚ŒãŸç¯„å›² ({min_size}x{min_size}ã€œ{max_size}x{max_size}) ã§å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    st.info(f"å®Ÿè¡Œã™ã‚‹å•é¡Œæ•°: {len(problem_files)}")
    progress_bar = st.progress(0, text="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œæº–å‚™ä¸­...")
    results = []
    start_time = time.time()

    for i, problem_path in enumerate(problem_files):
        progress = (i + 1) / len(problem_files)
        progress_bar.progress(progress, text=f"å®Ÿè¡Œä¸­ ({i + 1}/{len(problem_files)}): {problem_path.name}")
        result, error = run_single_problem(executable_path, str(problem_path))
        if result:
            try:
                size_str = problem_path.parts[-2]
                result['problem_id'] = problem_path.stem
                result['size'] = int(size_str.split('x')[0])
                results.append(result)
            except (IndexError, ValueError) as e:
                st.warning(f"ãƒ‘ã‚¹ '{problem_path}' ã‹ã‚‰ã‚µã‚¤ã‚ºã¾ãŸã¯IDã®è§£æã«å¤±æ•—: {e}")
        else:
            st.warning(f"å•é¡Œ {problem_path.name} ã§ã‚¨ãƒ©ãƒ¼: {error}")
            # å¤±æ•—ã—ãŸã‚±ãƒ¼ã‚¹ã‚‚è¨˜éŒ²ã™ã‚‹
            try:
                size_str = problem_path.parts[-2]
                size = int(size_str.split('x')[0])
                results.append({'solved': False, 'size': size, 'problem_id': problem_path.stem, 'error': error})
            except (IndexError, ValueError):
                results.append({'solved': False, 'size': np.nan, 'problem_id': problem_path.stem, 'error': error})

    end_time = time.time()
    if results:
        df = pd.DataFrame(results)
        df = calculate_metrics(df.copy())
        summary = calculate_summary(df)

        # ã€è¦ä»¶1, 2ã€‘çµæœã®ä¿å­˜å…ˆã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        executable_name = Path(executable_path).stem
        base_filename = f"{executable_name}_{timestamp}"

        results_csv_path = os.path.join(results_dir, f"{base_filename}.csv")
        results_json_path = os.path.join(results_dir, f"{base_filename}.json")

        df.to_csv(results_csv_path, index=False)
        if summary:
            # è¨­å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’å–å¾—ã—ã¦ã‚µãƒãƒªãƒ¼ã«è¿½åŠ 
            config_path = get_timeout_config_path(executable_path)
            configured_timeout = load_timeout_from_config(config_path)
            summary['timeout_seconds'] = configured_timeout # Use a more descriptive key like 'timeout_seconds'

            with open(results_json_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=to_serializable)

        progress_bar.progress(1.0, text="å®Œäº†ï¼")
        st.success(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå®Œäº†ï¼ (åˆè¨ˆæ™‚é–“: {end_time - start_time:.2f}ç§’)")
        st.success(f"çµæœã¯ '{results_csv_path}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        load_benchmark_results.clear()
        st.rerun()
    else:
        st.error("å®Ÿè¡Œã«æˆåŠŸã—ãŸå•é¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


def display_metrics_overview(summary, header="ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦"):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦ã‚’è¡¨ç¤º"""
    st.header(header)
    if not summary:
        st.warning("ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    col1, col2, col3, col4 = st.columns(4)
    solved_percentage = (summary.get('solved_problems', 0) / summary.get('total_problems', 1) * 100)

    with col1:
        st.markdown(f"""<div class="metric-card success-metric">
            <h3>è§£æ±ºç‡</h3><h2>{summary.get('solved_problems', 0)}/{summary.get('total_problems', 0)}</h2>
            <p>{solved_percentage:.1f}%</p></div>""", unsafe_allow_html=True)
    with col2:
        st.markdown(f"""<div class="metric-card info-metric">
            <h3>å¹³å‡è¨ˆç®—æ™‚é–“</h3><h2>{summary.get('avg_calculation_time_ms', 0):.2f}ms</h2>
            <p>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å†…éƒ¨</p></div>""", unsafe_allow_html=True)
    with col3:
        st.markdown(f"""<div class="metric-card warning-metric">
            <h3>å¹³å‡æ¢ç´¢ãƒãƒ¼ãƒ‰</h3><h2>{summary.get('avg_nodes_explored', 0):,.0f}</h2>
            <p>nodes</p></div>""", unsafe_allow_html=True)
    with col4:
        st.markdown(f"""<div class="metric-card">
            <h3>å¹³å‡æ‰‹æ•°</h3><h2>{summary.get('avg_moves', 0):.1f}</h2>
            <p>moves</p></div>""", unsafe_allow_html=True)


# --- å¯è¦–åŒ–é–¢æ•°ç¾¤ (å¤‰æ›´ãªã—) ---
def plot_time_performance(df):
    st.subheader("â±ï¸ æ™‚é–“æ€§èƒ½åˆ†æ")
    col1, col2 = st.columns(2)
    with col1:
        size_time = df.groupby('size')['calculation_time_ms'].agg(['mean', 'std']).reset_index()
        fig = px.line(size_time, x='size', y='mean', title='ã‚µã‚¤ã‚ºåˆ¥å¹³å‡è¨ˆç®—æ™‚é–“',
                      labels={'mean': 'å¹³å‡è¨ˆç®—æ™‚é–“ (ms)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        fig = px.histogram(df, x='calculation_time_ms', title='è¨ˆç®—æ™‚é–“åˆ†å¸ƒ',
                           labels={'calculation_time_ms': 'è¨ˆç®—æ™‚é–“ (ms)', 'count': 'å•é¡Œæ•°'}, nbins=50)
        st.plotly_chart(fig, use_container_width=True)


def plot_search_performance(df):
    st.subheader("ğŸ” æ¢ç´¢æ€§èƒ½åˆ†æ")
    col1, col2 = st.columns(2)
    with col1:
        fig = px.box(df, x='size', y='nodes_explored', title='ã‚µã‚¤ã‚ºåˆ¥æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°åˆ†å¸ƒ',
                     labels={'nodes_explored': 'æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º'})
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        fig = px.scatter(df, x='size', y='search_efficiency', color='calculation_time_ms',
                         title='æ¢ç´¢åŠ¹ç‡ vs ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                         labels={'search_efficiency': 'æ¢ç´¢åŠ¹ç‡ (nodes/sec)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'calculation_time_ms': 'è¨ˆç®—æ™‚é–“ (ms)'}, color_continuous_scale='viridis')
        st.plotly_chart(fig, use_container_width=True)


def plot_solution_quality(df):
    st.subheader("ğŸ¯ è§£ã®å“è³ªåˆ†æ")
    if 'diff_from_estimated' not in df.columns:
        st.info("è§£ã®å“è³ªã‚’åˆ†æã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return
    col1, col2 = st.columns(2)
    with col1:
        fig = px.scatter(df, x='size', y='diff_from_estimated', color='num_moves', title='æ¨å®šæœ€é©è§£(3nÂ²/8)ã‹ã‚‰ã®å·®åˆ†',
                         labels={'diff_from_estimated': 'å·®åˆ† (moves)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'num_moves': 'å®Ÿéš›ã®æ‰‹æ•°'}, color_continuous_scale='RdYlBu_r')
        fig.add_hline(y=0, line_dash="dash", line_color="red", annotation_text="ç†è«–æœ€é©å€¤")
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        fig = px.scatter(df, x='size', y='diff_from_n_squared', color='composite_score',
                         title='ç†è«–æœ€å¤§å€¤(nÂ²)ã‹ã‚‰ã®å·®åˆ†',
                         labels={'diff_from_n_squared': 'å·®åˆ† (moves)', 'size': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º',
                                 'composite_score': 'ç·åˆã‚¹ã‚³ã‚¢'}, color_continuous_scale='viridis')
        st.plotly_chart(fig, use_container_width=True)


def plot_comprehensive_analysis(df):
    st.subheader("ğŸ“ˆ ç·åˆåˆ†æ")
    st.info("å„æŒ‡æ¨™ã‚’0-100ã®ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã—ã¦è¡¨ç¤ºã€‚100ã«è¿‘ã„ã»ã©è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚")
    agg_df = df.dropna(subset=['calculation_time_ms', 'nodes_explored', 'num_moves'])
    if agg_df.empty:
        st.warning("ç·åˆåˆ†æã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    size_summary = agg_df.groupby('size').agg(
        calculation_time_ms=('calculation_time_ms', 'mean'),
        nodes_explored=('nodes_explored', 'mean'),
        num_moves=('num_moves', 'mean'),
        search_efficiency=('search_efficiency', 'mean'),
        composite_score=('composite_score', 'mean')
    ).reset_index()

    metrics_to_scale = {
        'calculation_time_ms': 'lower_is_better', 'nodes_explored': 'lower_is_better',
        'num_moves': 'lower_is_better', 'composite_score': 'lower_is_better',
        'search_efficiency': 'higher_is_better'
    }
    scaled_summary = size_summary.copy()
    for metric, scale_type in metrics_to_scale.items():
        if metric in scaled_summary.columns:
            min_val, max_val = scaled_summary[metric].min(), scaled_summary[metric].max()
            if (max_val - min_val) == 0:
                scaled_summary[metric] = 100.0
                continue
            if scale_type == 'lower_is_better':
                scaled_summary[metric] = 100 * (1 - (scaled_summary[metric] - min_val) / (max_val - min_val))
            else:
                scaled_summary[metric] = 100 * ((scaled_summary[metric] - min_val) / (max_val - min_val))

    display_metrics = {
        'calculation_time_ms': 'æ™‚é–“åŠ¹ç‡ã‚¹ã‚³ã‚¢', 'nodes_explored': 'æ¢ç´¢ç©ºé–“ã‚¹ã‚³ã‚¢',
        'num_moves': 'è§£å“è³ªã‚¹ã‚³ã‚¢', 'search_efficiency': 'æ¢ç´¢åŠ¹ç‡ã‚¹ã‚³ã‚¢',
        'composite_score': 'ç·åˆã‚¹ã‚³ã‚¢'
    }
    available_metrics = [m for m in display_metrics.keys() if m in scaled_summary.columns]
    if available_metrics:
        heatmap_data = scaled_summary.set_index('size')[available_metrics].T.rename(index=display_metrics)
        fig = px.imshow(heatmap_data, title='ã‚µã‚¤ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
                        labels={'x': 'ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º', 'y': 'è©•ä¾¡æŒ‡æ¨™', 'color': 'ã‚¹ã‚³ã‚¢ (0-100)'},
                        color_continuous_scale='RdYlGn', aspect="auto")
        st.plotly_chart(fig, use_container_width=True)


def display_detailed_table(df):
    st.subheader("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿")
    if df.empty:
        st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    col1, col2, col3 = st.columns(3)
    with col1:
        size_filter = st.multiselect("ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚ºé¸æŠ", options=sorted(df['size'].dropna().unique().astype(int)),
                                     default=sorted(df['size'].dropna().unique().astype(int)))
    with col2:
        min_time, max_time = float(df['calculation_time_ms'].min()), float(df['calculation_time_ms'].max())
        time_range = st.slider("è¨ˆç®—æ™‚é–“ç¯„å›² (ms)", min_value=min_time, max_value=max_time, value=(min_time, max_time))
    with col3:
        solved_filter = st.selectbox("è¡¨ç¤ºå¯¾è±¡", options=["ã™ã¹ã¦", "è§£æ±ºæ¸ˆã¿ã®ã¿", "å¤±æ•—ã®ã¿"], index=0)

    filtered_df = df[df['size'].isin(size_filter)]
    solved_part = filtered_df[filtered_df['solved'] == True]
    failed_part = filtered_df[filtered_df['solved'] == False]
    solved_part = solved_part[
        (solved_part['calculation_time_ms'] >= time_range[0]) & (solved_part['calculation_time_ms'] <= time_range[1])]
    filtered_df = pd.concat([solved_part, failed_part])

    if solved_filter == "è§£æ±ºæ¸ˆã¿ã®ã¿":
        filtered_df = filtered_df[filtered_df['solved'] == True]
    elif solved_filter == "å¤±æ•—ã®ã¿":
        filtered_df = filtered_df[filtered_df['solved'] == False]

    display_columns = ['size', 'problem_id', 'solved', 'num_moves', 'calculation_time_ms', 'nodes_explored',
                       'search_efficiency', 'diff_from_estimated', 'composite_score', 'error']
    available_columns = [col for col in display_columns if col in filtered_df.columns]
    st.dataframe(filtered_df[available_columns].fillna('N/A').round(4), use_container_width=True, height=400)
    st.write(f"è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿æ•°: {len(filtered_df)} / {len(df)}")


def individual_analysis_page(csv_files):
    """ã€æ–°è¦ã€‘å€‹åˆ¥åˆ†æãƒšãƒ¼ã‚¸ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯"""
    st.sidebar.title("âš™ï¸ æ“ä½œãƒ‘ãƒãƒ«")

    # --- ãƒ‡ãƒ¼ã‚¿é¸æŠ ---
    if not csv_files:
        st.warning("âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.info("ä¸‹ã®ã€Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€ã¾ãšãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        # ã€è¦ä»¶3ã€‘ resultså†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’é¸ã‚“ã§è¡¨ç¤º
        file_options = {Path(f).stem: f for f in csv_files}
        selected_option = st.sidebar.selectbox("åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ", options=list(file_options.keys()))
        selected_csv_path = file_options[selected_option]

        df, summary = load_benchmark_results(selected_csv_path)

        if df.empty:
            st.error("é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return

        st.sidebar.success(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {selected_option}")
        st.sidebar.info(f"å•é¡Œæ•°: {len(df)}")
        st.sidebar.info(f"ã‚µã‚¤ã‚ºç¯„å›²: {df['size'].min()}x{df['size'].min()} - {df['size'].max()}x{df['size'].max()}")

        if summary:
            display_metrics_overview(summary)

        tab1, tab2, tab3, tab4, tab5 = st.tabs(["â±ï¸ æ™‚é–“æ€§èƒ½", "ğŸ” æ¢ç´¢æ€§èƒ½", "ğŸ¯ è§£å“è³ª", "ğŸ“ˆ ç·åˆåˆ†æ", "ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿"])
        with tab1:
            plot_time_performance(df[df['solved'] == True])
        with tab2:
            plot_search_performance(df[df['solved'] == True])
        with tab3:
            plot_solution_quality(df[df['solved'] == True])
        with tab4:
            plot_comprehensive_analysis(df[df['solved'] == True])
        with tab5:
            display_detailed_table(df)

    # --- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ ---
    with st.sidebar.expander("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ", expanded=not csv_files):
        executable_path = st.text_input("å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹", value="./astar_manhattan")
        problems_dir = st.text_input("å•é¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", value="problems")

        min_avail, max_avail = 4, 24
        problem_path_obj = Path(problems_dir)
        if problem_path_obj.exists() and problem_path_obj.is_dir():
            available_sizes = [int(d.name.split('x')[0]) for d in problem_path_obj.iterdir() if
                               d.is_dir() and d.name.split('x')[0].isdigit()]
            if available_sizes:
                min_avail, max_avail = min(available_sizes), max(available_sizes)

        col1, col2 = st.columns(2)
        min_size = col1.number_input("æœ€å°ã‚µã‚¤ã‚º", min_value=min_avail, max_value=max_avail, value=min_avail, step=1)
        max_size = col2.number_input("æœ€å¤§ã‚µã‚¤ã‚º", min_value=min_avail, max_value=max_avail, value=max_avail, step=1)
        if min_size > max_size: st.error("æœ€å°ã‚µã‚¤ã‚ºãŒæœ€å¤§ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚")

        problems_per_size = st.number_input("å„ã‚µã‚¤ã‚ºã§å®Ÿè¡Œã™ã‚‹å•é¡Œæ•°", min_value=1, value=5, step=1)
        timeout_seconds = st.number_input("ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•° (10-300ç§’)", min_value=10, max_value=300, value=60, step=1, key="timeout_seconds_input")

        if st.button("ğŸš€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ", type="primary", disabled=(min_size > max_size)):
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’ä¿å­˜
            current_executable_path = executable_path # st.text_inputã®ç¾åœ¨ã®å€¤ã‚’å–å¾—
            config_path = get_timeout_config_path(current_executable_path)
            save_timeout_to_config(config_path, timeout_seconds)

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            run_benchmark_with_config(current_executable_path, problems_dir, problems_per_size, min_size, max_size)


# --- Helper Functions for Comparison Page ---

def load_and_merge_benchmark_data(
    selected_options: list[str],
    file_map: dict[str, str]
) -> tuple[pd.DataFrame | None, list[str], dict[str, dict]]:
    """Loads and merges data for selected benchmarks."""
    all_dfs = []
    all_df_names = []
    all_summaries = {}

    for option_stem in selected_options:
        filepath = file_map[option_stem]
        df, summary = load_benchmark_results(filepath) # Assuming load_benchmark_results is defined elsewhere
        if df is not None and not df.empty:
            # Ensure 'problem_id' is string for consistent merging
            if 'problem_id' in df.columns:
                df['problem_id'] = df['problem_id'].astype(str)
            if 'size' in df.columns: # Ensure size is int
                 df['size'] = df['size'].astype(int)

            all_dfs.append(df)
            all_df_names.append(option_stem)
            if summary is not None:
                all_summaries[option_stem] = summary
        else:
            st.warning(f"çµæœ '{option_stem}' ã®ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸã‹ã€ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    if len(all_dfs) < 1: # Need at least one to start merging/displaying
        st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ1ã¤ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, [], {}

    if not all_dfs: # Should be caught by len(all_dfs) < 1 already
        return None, [], {}

    # Merge DataFrames
    merged_df = all_dfs[0].copy()
    # Ensure 'solved' column is boolean and handle potential non-boolean types before suffixing.
    # Also, make sure essential merge keys 'size', 'problem_id' are present.
    if not all({'size', 'problem_id'}.issubset(merged_df.columns)):
        st.error(f"æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  '{all_df_names[0]}' ã« 'size' ã¾ãŸã¯ 'problem_id' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ¼ã‚¸ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        return None, [], {}

    if f"solved" in merged_df.columns: # Original column name before suffix
        merged_df[f"solved"] = merged_df[f"solved"].astype('boolean')
    merged_df = merged_df.add_suffix(f"_{all_df_names[0]}")
    # Rename merge keys to not have suffix from the first df
    merged_df.rename(columns={
        f"size_{all_df_names[0]}": "size",
        f"problem_id_{all_df_names[0]}": "problem_id"
    }, inplace=True)


    for i in range(1, len(all_dfs)):
        current_df_name = all_df_names[i]
        current_df_to_merge = all_dfs[i].copy()

        if not all({'size', 'problem_id'}.issubset(current_df_to_merge.columns)):
            st.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  '{current_df_name}' ã« 'size' ã¾ãŸã¯ 'problem_id' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        if f"solved" in current_df_to_merge.columns:
            current_df_to_merge[f"solved"] = current_df_to_merge[f"solved"].astype('boolean')
        current_df_to_merge = current_df_to_merge.add_suffix(f"_{current_df_name}")
        current_df_to_merge.rename(columns={
            f"size_{current_df_name}": "size",
            f"problem_id_{current_df_name}": "problem_id"
        }, inplace=True)

        merged_df = pd.merge(merged_df, current_df_to_merge, on=['size', 'problem_id'], how='outer')

    return merged_df, all_df_names, all_summaries


def calculate_benchmark_improvement_rates(
    merged_df: pd.DataFrame,
    baseline_name: str,
    all_df_names: list[str],
    metrics_to_compare: list[str]
) -> pd.DataFrame:
    """Calculates improvement rates for specified metrics against a baseline."""
    if not baseline_name or merged_df.empty:
        return merged_df

    # Make a copy to avoid SettingWithCopyWarning
    # merged_df_processed = merged_df.copy() # Already a copy if coming from load_and_merge
    # No, merged_df from load_and_merge is the actual merged_df, so copy is good.
    merged_df_processed = merged_df.copy()


    for df_name in all_df_names:
        if df_name == baseline_name:
            continue

        for metric in metrics_to_compare:
            baseline_col = f"{metric}_{baseline_name}"
            current_col = f"{metric}_{df_name}"
            rate_col_name = f"{metric}_improvement_rate_vs_{baseline_name}_for_{df_name}"

            if baseline_col in merged_df_processed.columns and current_col in merged_df_processed.columns:
                baseline_values = merged_df_processed[baseline_col].astype(float)
                current_values = merged_df_processed[current_col].astype(float)

                improvement_rate = (baseline_values - current_values) / baseline_values * 100

                improvement_rate[ (baseline_values == 0) & (current_values == 0) ] = 0
                improvement_rate[ (baseline_values == 0) & (current_values > 0) ] = -np.inf

                improvement_rate.replace([np.inf, -np.inf], np.nan, inplace=True)
                merged_df_processed[rate_col_name] = improvement_rate # Keep NaNs for now
            else:
                st.warning(f"ãƒ¬ãƒ¼ãƒˆè¨ˆç®—ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {baseline_col} ã¾ãŸã¯ {current_col}")

    return merged_df_processed


def create_comparison_summary_table_df(
    all_summaries: dict[str, dict],
    merged_df: pd.DataFrame,
    all_df_names: list[str],
    baseline_name: str | None,
    basic_metric_keys: dict[str, str], # e.g. {'total_problems': 'Total Problems'}
    rate_metric_specs: dict[str, str] # e.g. {'calculation_time_ms': 'Avg Calc Time Improvement %'}
) -> pd.DataFrame:
    """Creates the summary table DataFrame for comparison."""

    table_rows = []

    # Basic summary metrics
    for internal_key, display_name in basic_metric_keys.items():
        row_data = {'Metric': display_name}
        for df_name in all_df_names:
            summary = all_summaries.get(df_name, {})
            val = summary.get(internal_key)
            if pd.isna(val):
                row_data[df_name] = "N/A"
            elif isinstance(val, float):
                if 'time' in internal_key or 'ms' in internal_key : # crude check for time
                     row_data[df_name] = f"{val:.2f}"
                elif 'nodes' in internal_key :
                     row_data[df_name] = f"{val:,.0f}"
                elif 'moves' in internal_key:
                     row_data[df_name] = f"{val:.1f}"
                else:
                     row_data[df_name] = f"{val:.2f}" # Default float formatting
            else: # int or string
                row_data[df_name] = val
        table_rows.append(row_data)

    # Improvement rate metrics
    if baseline_name and not merged_df.empty:
        for internal_metric, display_name in rate_metric_specs.items():
            row_data = {'Metric': display_name}
            for df_name in all_df_names:
                if df_name == baseline_name:
                    row_data[df_name] = "Baseline"
                else:
                    rate_col = f"{internal_metric}_improvement_rate_vs_{baseline_name}_for_{df_name}"
                    if rate_col in merged_df.columns:
                        avg_rate = merged_df[rate_col].mean()
                        row_data[df_name] = f"{avg_rate:.1f}%" if pd.notna(avg_rate) else "N/A"
                    else:
                        row_data[df_name] = "N/A (no data)"
            table_rows.append(row_data)

    if not table_rows: # If no data at all
        return pd.DataFrame(columns=['Metric'] + all_df_names).set_index('Metric')

    summary_df = pd.DataFrame(table_rows)
    summary_df.set_index('Metric', inplace=True)
    return summary_df


def generate_comparison_line_charts_figures(
    merged_df: pd.DataFrame,
    all_df_names: list[str],
    chart_specs: list[dict] # Each dict: {'metric_key': 'col_name_part', 'title': 'Chart Title', 'yaxis_title': 'Y Axis'}
) -> list[go.Figure]:
    """Generates line charts for comparing benchmark performance metrics."""
    figures = []
    if merged_df.empty:
        return figures

    for spec in chart_specs:
        fig = go.Figure()
        st.subheader(spec['title']) # Display title before the chart

        for df_name in all_df_names:
            solved_col = f"solved_{df_name}"
            metric_col = f"{spec['metric_key']}_{df_name}"

            if solved_col in merged_df.columns and metric_col in merged_df.columns:
                # Ensure boolean type for solved_col before filtering
                # This might be redundant if already cast during merge, but safe.
                try: # Add try-except for astype if column is all NaN from an outer join
                    merged_df[solved_col] = merged_df[solved_col].astype('boolean')
                    solved_data_for_benchmark = merged_df[merged_df[solved_col].fillna(False)] # Treat NaN as False for solved
                except TypeError: # Handle cases where astype('boolean') fails e.g. mixed types not convertible
                     st.caption(f"æ³¨æ„: '{df_name}' ã®è§£æ±ºçŠ¶æ…‹ã‚«ãƒ©ãƒ  ({solved_col}) ã®å‹å¤‰æ›ã«å•é¡ŒãŒã‚ã‚Šã€ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                     continue


                if not solved_data_for_benchmark.empty:
                    agg_data = solved_data_for_benchmark.groupby('size')[metric_col].mean().reset_index()
                    fig.add_trace(go.Scatter(x=agg_data['size'], y=agg_data[metric_col], mode='lines+markers', name=df_name))
                else:
                    st.caption(f"æ³¨æ„: '{df_name}' ã«ã¯ '{spec['title']}' ã®è§£æ±ºæ¸ˆã¿å•é¡Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.caption(f"æ³¨æ„: '{df_name}' ã® '{spec['title']}' ã¾ãŸã¯è§£æ±ºçŠ¶æ…‹ã‚«ãƒ©ãƒ  ({metric_col} or {solved_col}) ãŒãƒãƒ¼ã‚¸å¾Œãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

        fig.update_layout(xaxis_title="ãƒ‘ã‚ºãƒ«ã‚µã‚¤ã‚º", yaxis_title=spec['yaxis_title'], legend_title_text='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
        figures.append(fig)

    return figures


# --- Main Application Pages ---

def comparison_page(csv_files):
    """æ¯”è¼ƒåˆ†æãƒšãƒ¼ã‚¸ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯"""
    st.title("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")

    if len(csv_files) < 2:
        st.warning("æ¯”è¼ƒã™ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒå¿…è¦ã§ã™ã€‚")
        st.info("ã€Œå€‹åˆ¥åˆ†æã€ãƒšãƒ¼ã‚¸ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’2å›ä»¥ä¸Šå®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    file_options = {Path(f).stem: f for f in csv_files}
    selected_benchmark_options = st.multiselect(
        "æ¯”è¼ƒã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’é¸æŠ (2ã¤ä»¥ä¸Š)",
        options=list(file_options.keys()),
        key="benchmark_multiselect"
    )

    if len(selected_benchmark_options) < 2:
        st.warning("æ¯”è¼ƒã™ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return

    # 1. Load and Merge Data
    merged_df, all_df_names, all_summaries = load_and_merge_benchmark_data(
        selected_benchmark_options, file_options
    )

    if merged_df is None or merged_df.empty:
        st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚")
        return
    if len(all_df_names) < 2: # Check again after loading in case some failed
        st.warning("æ¯”è¼ƒã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€å°‘ãªãã¨ã‚‚2ã¤ã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        return


    # 2. Baseline Selection and Rate Calculation
    st.markdown("---")
    baseline_name = st.selectbox(
        "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é¸æŠ",
        options=all_df_names, # Use names of successfully loaded DFs
        index=0,
        key="baseline_selection"
    )

    metrics_for_rate_calculation = ['calculation_time_ms', 'nodes_explored']
    if baseline_name:
        merged_df = calculate_benchmark_improvement_rates(
            merged_df, baseline_name, all_df_names, metrics_for_rate_calculation
        )

    # 3. Display Summary Table
    st.markdown("---")
    st.subheader("ğŸ“Š ç·åˆã‚µãƒãƒªãƒ¼æ¯”è¼ƒ")
    basic_summary_metrics = {
        'total_problems': 'Total Problems',
        'solved_problems': 'Solved Problems',
        'avg_calculation_time_ms': 'Avg Calculation Time (ms)',
        'avg_nodes_explored': 'Avg Nodes Explored',
        'avg_moves': 'Avg Moves'
    }
    rate_metrics_for_summary = {
        'calculation_time_ms': 'Avg Calc Time Improvement %',
        'nodes_explored': 'Avg Nodes Explored Impr. %'
    }
    summary_table_df = create_comparison_summary_table_df(
        all_summaries, merged_df, all_df_names, baseline_name,
        basic_summary_metrics, rate_metrics_for_summary
    )
    st.dataframe(summary_table_df, use_container_width=True)

    # 4. Display Charts
    st.markdown("---")
    st.header("ğŸ“ˆ ã‚°ãƒ©ãƒ•ã§ã®æ€§èƒ½æ¯”è¼ƒ")

    chart_specs = [
        {'metric_key': 'calculation_time_ms', 'title': 'â±ï¸ ã‚µã‚¤ã‚ºåˆ¥å¹³å‡è¨ˆç®—æ™‚é–“', 'yaxis_title': 'å¹³å‡è¨ˆç®—æ™‚é–“ (ms)'},
        {'metric_key': 'nodes_explored', 'title': 'ğŸ” ã‚µã‚¤ã‚ºåˆ¥å¹³å‡æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°', 'yaxis_title': 'å¹³å‡æ¢ç´¢ãƒãƒ¼ãƒ‰æ•°'}
    ]

    if not merged_df.empty:
        chart_figures = generate_comparison_line_charts_figures(merged_df, all_df_names, chart_specs)
        for fig in chart_figures:
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    # Debug view
    if st.checkbox("ãƒãƒ¼ã‚¸ã•ã‚ŒãŸDataFrameã‚’è¡¨ç¤º (ãƒ‡ãƒãƒƒã‚°ç”¨)"):
        st.dataframe(merged_df)
        st.write(f"ã‚«ãƒ©ãƒ å: {merged_df.columns.tolist()}")


def main():
    """ã€å¤‰æ›´ã€‘ãƒ¡ã‚¤ãƒ³é–¢æ•°: ãƒšãƒ¼ã‚¸é¸æŠã¨ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ"""
    st.sidebar.title("ğŸ“„ ãƒšãƒ¼ã‚¸é¸æŠ")
    page = st.sidebar.radio("è¡¨ç¤ºã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’é¸æŠ", ["å€‹åˆ¥åˆ†æ", "æ¯”è¼ƒåˆ†æ"], label_visibility="collapsed")

    st.sidebar.markdown("---")

    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    csv_files = sorted(glob.glob(os.path.join(results_dir, "*.csv")), reverse=True)

    st.title("ğŸ” é«˜å°‚ãƒ—ãƒ­ã‚³ãƒ³ç«¶æŠ€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

    if page == "å€‹åˆ¥åˆ†æ":
        individual_analysis_page(csv_files)
    elif page == "æ¯”è¼ƒåˆ†æ":
        comparison_page(csv_files)

    st.markdown("---")
    st.markdown("ğŸš€ A*ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  | Built with Streamlit")


if __name__ == "__main__":
    main()